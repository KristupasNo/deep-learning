{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Kristupas Norvaiša, 2016012, II grupė"
      ],
      "metadata": {
        "id": "2Wc9hIxSmh73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "RuKFpmlSl5eU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-nYeqfeN7B3",
        "outputId": "ce6af71e-ce0b-4df1-9f89-00136a3cfe2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openimages\n",
            "  Downloading openimages-0.0.1-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openimages) (4.65.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from openimages) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from openimages) (2.27.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from openimages) (4.9.2)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.111-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cvdata\n",
            "  Downloading cvdata-0.0.3-py3-none-any.whl (37 kB)\n",
            "Collecting botocore<1.30.0,>=1.29.111\n",
            "  Downloading botocore-1.29.111-py3-none-any.whl (10.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from cvdata->openimages) (8.4.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (from cvdata->openimages) (4.7.0.72)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from cvdata->openimages) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->openimages) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->openimages) (2022.7.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->openimages) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->openimages) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->openimages) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->openimages) (1.26.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->openimages) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, cvdata, openimages\n",
            "Successfully installed boto3-1.26.111 botocore-1.29.111 cvdata-0.0.3 jmespath-1.0.1 openimages-0.0.1 s3transfer-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openimages\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
        "from openimages.download import download_dataset\n",
        "import os\n",
        "if not torch.cuda.is_available():\n",
        "    device = torch.device(\"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining convolutional network"
      ],
      "metadata": {
        "id": "9G_LD5E6l821"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConvNet(nn.Module):\n",
        "    def __init__(self, in_shape, out_classes):\n",
        "        super().__init__()\n",
        "        self.num_classes = out_classes\n",
        "        self.conv1_1 = nn.Conv2d(in_shape[0], 8, (3, 3), padding = 'same')\n",
        "        self.conv1_2 = nn.Conv2d(8, 8, (3, 3), padding = 'same')\n",
        "        self.conv2_1 = nn.Conv2d(8, 16, (3, 3), padding = 'same')\n",
        "        self.conv2_2 = nn.Conv2d(16, 16, (3, 3), padding = 'same')\n",
        "        self.fc1 = nn.Linear(16 * (in_shape[1] // 4) * (in_shape[2] // 4), 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, out_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = nn.Sequential(\n",
        "            self.conv1_1,\n",
        "            nn.ReLU(),\n",
        "            self.conv1_2,\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), (2, 2)),\n",
        "            self.conv2_1,\n",
        "            nn.ReLU(),\n",
        "            self.conv2_2,\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), (2, 2)),\n",
        "            nn.Flatten(),\n",
        "            self.fc1,\n",
        "            nn.ReLU(),\n",
        "            self.fc2,\n",
        "            nn.ReLU(),\n",
        "            self.fc3\n",
        "        )(x)\n",
        "        return y"
      ],
      "metadata": {
        "id": "5BVpBU7IOSJ-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and evaluation of the model, data preprocessing, metrics calculation"
      ],
      "metadata": {
        "id": "8PMEX3jRmGgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval(convnet, dl_train, dl_valid, n_epochs, criterion, optimizer):\n",
        "    for epoch in range(n_epochs):\n",
        "        convnet.train()\n",
        "        for images, labels in dl_train:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            predictions = convnet(images)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        convnet.eval()\n",
        "        true_labels = []\n",
        "        pred_labels = []\n",
        "        for images, labels in dl_valid:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            predictions = convnet(images)\n",
        "            _, preds = torch.max(predictions, 1)\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            pred_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "        acc = accuracy_score(true_labels, pred_labels)\n",
        "        prec, rec, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average=None)\n",
        "        cm = confusion_matrix(true_labels, pred_labels)\n",
        "        class_acc = cm.diagonal() / cm.sum(axis=1)\n",
        "        class_names = [\"Bee\", \"Goldfish\", \"Goose\"]\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{n_epochs} -- Accuracy: {acc:.3f} -- Precision: {prec} -- Recall: {rec} -- F1: {f1} -- Confusion Matrix:\\n{cm}\")\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            print(f\"Accuracy for {class_name}: {class_acc[i]:.3f}\")\n",
        "\n",
        "transform_train = T.Compose([\n",
        "    T.RandomRotation(10),\n",
        "    T.Resize((64, 64)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_valid = T.Compose([\n",
        "    T.Resize((64, 64)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "if not os.path.exists(\"images\"):\n",
        "    download_dataset(\"images\", [\"Bee\", \"Goldfish\", \"Goose\"], annotation_format=None, limit=10)\n",
        "\n",
        "train_data = datasets.ImageFolder('/content/images', transform=transform_train)\n",
        "valid_data = datasets.ImageFolder('/content/images', transform=transform_valid)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_data, batch_size=32, shuffle=False)\n",
        "\n",
        "input_shape = (3, 64, 64)\n",
        "output_classes = 3\n",
        "convnet = SimpleConvNet(input_shape, output_classes).to(device)\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(convnet.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 10\n",
        "train_and_eval(convnet, train_loader, valid_loader, epochs, loss_func, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuUc36JrOYLL",
        "outputId": "2f64b1ed-4346-4da8-ff14-ff262d8779e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 -- Accuracy: 0.333 -- Precision: [0.33333333 0.         0.        ] -- Recall: [1. 0. 0.] -- F1: [0.5 0.  0. ] -- Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [10  0  0]\n",
            " [10  0  0]]\n",
            "Accuracy for Bee: 1.000\n",
            "Accuracy for Goldfish: 0.000\n",
            "Accuracy for Goose: 0.000\n",
            "Epoch 2/10 -- Accuracy: 0.367 -- Precision: [1.         0.34482759 0.        ] -- Recall: [0.1 1.  0. ] -- F1: [0.18181818 0.51282051 0.        ] -- Confusion Matrix:\n",
            "[[ 1  9  0]\n",
            " [ 0 10  0]\n",
            " [ 0 10  0]]\n",
            "Accuracy for Bee: 0.100\n",
            "Accuracy for Goldfish: 1.000\n",
            "Accuracy for Goose: 0.000\n",
            "Epoch 3/10 -- Accuracy: 0.433 -- Precision: [0.    0.5   0.375] -- Recall: [0.  0.7 0.6] -- F1: [0.         0.58333333 0.46153846] -- Confusion Matrix:\n",
            "[[0 3 7]\n",
            " [0 7 3]\n",
            " [0 4 6]]\n",
            "Accuracy for Bee: 0.000\n",
            "Accuracy for Goldfish: 0.700\n",
            "Accuracy for Goose: 0.600\n",
            "Epoch 4/10 -- Accuracy: 0.333 -- Precision: [0.         0.         0.33333333] -- Recall: [0. 0. 1.] -- F1: [0.  0.  0.5] -- Confusion Matrix:\n",
            "[[ 0  0 10]\n",
            " [ 0  0 10]\n",
            " [ 0  0 10]]\n",
            "Accuracy for Bee: 0.000\n",
            "Accuracy for Goldfish: 0.000\n",
            "Accuracy for Goose: 1.000\n",
            "Epoch 5/10 -- Accuracy: 0.367 -- Precision: [1.         0.         0.34482759] -- Recall: [0.1 0.  1. ] -- F1: [0.18181818 0.         0.51282051] -- Confusion Matrix:\n",
            "[[ 1  0  9]\n",
            " [ 0  0 10]\n",
            " [ 0  0 10]]\n",
            "Accuracy for Bee: 0.100\n",
            "Accuracy for Goldfish: 0.000\n",
            "Accuracy for Goose: 1.000\n",
            "Epoch 6/10 -- Accuracy: 0.600 -- Precision: [1.         0.71428571 0.47368421] -- Recall: [0.4 0.5 0.9] -- F1: [0.57142857 0.58823529 0.62068966] -- Confusion Matrix:\n",
            "[[4 1 5]\n",
            " [0 5 5]\n",
            " [0 1 9]]\n",
            "Accuracy for Bee: 0.400\n",
            "Accuracy for Goldfish: 0.500\n",
            "Accuracy for Goose: 0.900\n",
            "Epoch 7/10 -- Accuracy: 0.500 -- Precision: [1.         0.41666667 0.5       ] -- Recall: [0.4 1.  0.1] -- F1: [0.57142857 0.58823529 0.16666667] -- Confusion Matrix:\n",
            "[[ 4  5  1]\n",
            " [ 0 10  0]\n",
            " [ 0  9  1]]\n",
            "Accuracy for Bee: 0.400\n",
            "Accuracy for Goldfish: 1.000\n",
            "Accuracy for Goose: 0.100\n",
            "Epoch 8/10 -- Accuracy: 0.467 -- Precision: [1.         0.38461538 0.        ] -- Recall: [0.4 1.  0. ] -- F1: [0.57142857 0.55555556 0.        ] -- Confusion Matrix:\n",
            "[[ 4  6  0]\n",
            " [ 0 10  0]\n",
            " [ 0 10  0]]\n",
            "Accuracy for Bee: 0.400\n",
            "Accuracy for Goldfish: 1.000\n",
            "Accuracy for Goose: 0.000\n",
            "Epoch 9/10 -- Accuracy: 0.467 -- Precision: [1.         0.38461538 0.        ] -- Recall: [0.4 1.  0. ] -- F1: [0.57142857 0.55555556 0.        ] -- Confusion Matrix:\n",
            "[[ 4  6  0]\n",
            " [ 0 10  0]\n",
            " [ 0 10  0]]\n",
            "Accuracy for Bee: 0.400\n",
            "Accuracy for Goldfish: 1.000\n",
            "Accuracy for Goose: 0.000\n",
            "Epoch 10/10 -- Accuracy: 0.500 -- Precision: [1.  0.4 0. ] -- Recall: [0.5 1.  0. ] -- F1: [0.66666667 0.57142857 0.        ] -- Confusion Matrix:\n",
            "[[ 5  5  0]\n",
            " [ 0 10  0]\n",
            " [ 0 10  0]]\n",
            "Accuracy for Bee: 0.500\n",
            "Accuracy for Goldfish: 1.000\n",
            "Accuracy for Goose: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image input to label output"
      ],
      "metadata": {
        "id": "gHbnY54HmYYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_picture(img):\n",
        "    convnet.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = convnet(transform_valid(img).unsqueeze(0).to(device))\n",
        "    label_pred = torch.argmax(pred, axis=1)\n",
        "\n",
        "    classes = [\"Bee\", \"Goldfish\", \"Goose\"]\n",
        "    return classes[label_pred]"
      ],
      "metadata": {
        "id": "WCcgXQKxkbjm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web application"
      ],
      "metadata": {
        "id": "ptBgdAP2mea9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask --quiet\n",
        "!pip install flask-ngrok --quiet\n",
        "print(\"Completed!\")\n",
        "\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz\n",
        "\n",
        "!tar -xvf /content/ngrok-stable-linux-amd64.tgz\n",
        "\n",
        "!./ngrok authtoken 2NvAbgVgYtGDBWBVC1xleWHamBO_234SofNE5TzUMKMMBGyx7\n",
        "\n",
        "from flask import Flask, render_template, request\n",
        "from PIL import Image as im\n",
        "import cv2\n",
        "\n",
        "# import run_with_ngrok from flask_ngrok to run the app using ngrok\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__, template_folder='/content')\n",
        "run_with_ngrok(app)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvsM4s5KkrlQ",
        "outputId": "56369166-741e-43c2-c457-dd603bbdc7e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed!\n",
            "--2023-04-12 06:52:33--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 18.205.222.128, 52.202.168.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13856790 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.tgz’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.21M  3.35MB/s    in 6.9s    \n",
            "\n",
            "2023-04-12 06:52:40 (1.91 MB/s) - ‘ngrok-stable-linux-amd64.tgz’ saved [13856790/13856790]\n",
            "\n",
            "ngrok\n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route(\"/\")\n",
        "def hello():\n",
        "    return render_template('index.html')"
      ],
      "metadata": {
        "id": "DFZWouf8kx1B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route('/', methods=['POST'])\n",
        "def upload_image():\n",
        "    filestr = request.files['file'].read()\n",
        "    file_bytes = np.fromstring(filestr, np.uint8)\n",
        "    img_array = cv2.imdecode(file_bytes, cv2.IMREAD_UNCHANGED)\n",
        "    img = im.fromarray(img_array)\n",
        "\n",
        "    return f\"<h1>The picture is probably: {predict_picture(img)}</h1>\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD2oYLpUkzkm",
        "outputId": "91108256-ad3e-4c60-ffc1-28a206e3f290"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://f57f-35-204-211-180.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/Apr/2023 06:53:38] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Apr/2023 06:53:39] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Apr/2023 06:54:08] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Apr/2023 06:55:27] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Apr/2023 06:55:29] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Apr/2023 06:55:46] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Apr/2023 06:57:05] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Apr/2023 06:57:13] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Apr/2023 06:57:43] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Apr/2023 06:57:48] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Apr/2023 06:57:53] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Apr/2023 06:58:24] \"POST / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Apr/2023 06:59:01] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Apr/2023 06:59:10] \"POST / HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    }
  ]
}